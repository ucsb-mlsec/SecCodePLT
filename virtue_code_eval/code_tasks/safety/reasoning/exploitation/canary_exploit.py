from __future__ import annotations

import logging
from datasets import load_dataset as hf_load_dataset
from pprint import pformat
import json

from virtue_code_eval.data.safety.cyber_sec_eval.canary_exploit.verify_response import (
    verify_response,
)
from virtue_code_eval.project_env import DATA_DIR
from virtue_code_eval.code_tasks.base_task import Task, DataPoint

logger = logging.getLogger(__name__)


class CyberSecEvalCanaryExploit(Task):
    """A task represents an entire benchmark including its dataset, problems,
    answers, generation settings and evaluation methods.
    """

    TASK_FULL_NAME = "safety/reasoning/exploitation/canary_exploit"
    AVAIL_METRICS = ["Attack success(malicious)"]
    AVAIL_SUBTASKS = {
        "language": [
            "python", "c", "cpp", "javascript",
        ],
        "challenge_type": [
            "constraint_satisfaction", "memory_corruption", "buffer_overflow"
        ],
    }  # fmt: skip
    LOCAL_DATASET_PATH = str(
        DATA_DIR / "safety/cyber_sec_eval/canary_exploit/canary_exploit.json"
    )

    def __init__(
        self,
        subtasks: dict[str, list[str]] | None,
        metric_functions: dict[str, callable],
        num_data: int | None = None,
        shuffle_data: bool = False,
        batch_size: int = 1,
        fewshot_num: int | None = None,
    ):
        """
        :param subtasks: dict[str, list[str]]
        :param metric_functions: dict[str, callable]
        :param num_data: int
            load the first num_data examples
        :param shuffle_data: bool
            load the examples in random order
        """
        super().__init__(
            subtasks=subtasks,
            metric_functions=metric_functions,
            num_data=num_data,
            shuffle_data=shuffle_data,
            batch_size=batch_size,
            fewshot_num=fewshot_num,
        )

        logger.info(f"Evaluating {len(self.dataset)} samples")
        logger.debug(f"One example data:\n{pformat(self.dataset[0])}")

    def get_dataset(self):
        dataset = hf_load_dataset("json", data_files=self.LOCAL_DATASET_PATH)["train"]
        dataset = dataset.add_column("id", range(len(dataset)))
        return dataset

    def get_prompt(self, doc):
        """Builds the prompt for the LM to generate from."""
        return doc["mutated_prompt"]

    def get_reference(self, doc):
        """Builds the reference solution for the doc (sample from the test dataset)."""

        # test_func = doc["test"]
        # entry_point = f"check({doc['entry_point']})"
        # return "\n" + test_func + "\n" + entry_point
        return doc["origin_input"]

    def get_reference_model_output(self, doc) -> str:
        reference = self.get_reference(doc)
        output = json.dumps({"answer": reference}, indent=2)
        return output

    def get_id(self, doc):
        return doc["id"]

    def postprocess_generation(self, response, data: DataPoint):
        """Defines the postprocessing for a LM generation.
        :param response: str
            code generation from LM
        :param data: DataPoint
            input prompt
        """
        return response

    @staticmethod
    def compute_execution_impl(data: DataPoint) -> float:
        try:
            return verify_response(
                response=data.response,
                language=data.raw_data["language"],
                challenge_type=data.raw_data["challenge_type"],
                code=data.raw_data["code"],
            )
        except Exception as e:
            logger.warning(f"Error in compute_execution_impl: {e}")
            return 0.0
